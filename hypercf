#!/usr/bin/env python -u
#!*-* coding:utf-8 *-*

import argparse
import os
import json
import urllib2
import multiprocessing
import requests
import signal
import time


########################################################################
class Authentication(object):

    #----------------------------------------------------------------------
    def cloud_files_service(self, args=None):
        auth_url = 'https://identity.api.rackspacecloud.com/v2.0/tokens'
        json_params = json.dumps({'auth': {'RAX-KSKEY:apiKeyCredentials': {'username': args['user'], 'apiKey': args['key']}}})
        headers = ({'Content-Type': 'application/json'}) 
        req = requests.post(auth_url, data=json_params, headers=headers)
        json_response = json.loads(req.text)
        auth_details = {}
        try:
            catalogs = json_response['access']['serviceCatalog']
            for service in catalogs:
                if service['name'] == 'cloudFiles':
                    for region in service['endpoints']:
                        if region['region'] == args['reg']:
                            auth_details['tenantid'] = region['tenantId']
                            auth_details['region'] = region['publicURL']
                            if args['snet']:
                                auth_details['region'] = region['internalURL']
            auth_details['token'] = json_response['access']['token']['id']
        except(KeyError, IndexError):
            print 'Authenticaton Error: Unable to continue.'
        return auth_details


########################################################################
class Requests(object):

    #----------------------------------------------------------------------
    def __init__(self, container):
        pass

    #----------------------------------------------------------------------
    def run_queries(self, url, headers, requestType, streamVal='False', data=None):
        global auth_data
        global dict_args
        if requestType == requests.put and data:
            data_val = open(data, "rb")
        elif requestType == requests.delete and data:
            data_val = data
        else:
            data_val = None
        while url:
            resp = requestType(url, headers=headers, stream=streamVal, data=data_val)
            rcode = resp.status_code

            if rcode == 401:
                print 'Error 401, re-authenticating'
                auth_data = Authentication().cloud_files_service(dict_args)
                continue
            elif rcode == 404:
                print ('Error 404 Not Found' + url)
                return
            elif rcode >= 300:
                exit('Error communicating with CloudFiles %s' % rcode)
            else:
                if requestType == requests.get and data:
                    with open(data, "wb") as local_file:
                        local_file.write(resp.content)
                        local_file.close()
                return resp


########################################################################
class Consumer(multiprocessing.Process):
    #----------------------------------------------------------------------
    def __init__(self, task_queue, result_queue):
        multiprocessing.Process.__init__(self)
        self.task_queue = task_queue
        self.result_queue = result_queue
        self.qcount = 0

    def run(self):
        while True:
            next_task = self.task_queue.get()
            if next_task is None:
                break
            answer = next_task()
            self.result_queue.put(answer)
        return


########################################################################
class Task(object):
    def __init__(self, var, prog_count, target_def):
        self.var = var
        self.prog_count = prog_count
        self.target_def = target_def

    def __call__(self):
        self.target_def(self.var, self.prog_count)
        return '%s * %s = %s' % (self.var, self.prog_count, self.prog_count)

    def __str__(self):
        return '%s * %s' % (self.var, self.prog_count)


#----------------------------------------------------------------------
def collect_containers():
    global auth_data
    global authority
    last_container = ''
    cont_total = 0
    cont_count = 0
    gather_conts = True
    find_cont_total = True
    container_list = []
    while gather_conts:
        url = (authority + '/?limit=10000&format=json')
        if last_container:
            url = (url + '&marker=' + urllib2.quote(last_container))
        headers = {'X-Auth-Token': auth_data['token'],  'Connection': 'Keep-Alive', 'Content-Type': 'application/json'}
        req_obj = Requests(0)
        resp = req_obj.run_queries(url, headers, requests.get)
        json_response = json.loads(resp.text)
        if find_cont_total:
            cont_total = int(resp.headers['X-Account-Container-Count'])
            find_cont_total = False
            if cont_total == 0:
                return

        if len(json_response) > 0:
            for cont in json_response:
                cont_count += 1
                container_list.append(cont['name'].encode('utf-8'))
                last_container = container_list[-1]
                if cont_count == cont_total:
                    return container_list


#----------------------------------------------------------------------
def collect_container_objects():
    global auth_data
    global obj_total
    global authority
    last_object = ''
    container = args.cont.encode('utf-8')
    obj_total = 0
    obj_count = 0
    gather_objs = True
    find_obj_total = True
    object_list = []
    while gather_objs:
        url = (authority + '/' + urllib2.quote(container) + '/?limit=10000&format=json')
        if last_object:
            url = (url + '&marker=' + urllib2.quote(last_object))
        headers = {'X-Auth-Token': auth_data['token'],  'Connection': 'Keep-Alive', 'Content-Type': 'application/json'}
        req_obj = Requests(container)
        resp = req_obj.run_queries(url, headers, requests.get)
        json_response = json.loads(resp.text)
        if find_obj_total:
            obj_total = int(resp.headers['X-Container-Object-Count'])
            find_obj_total = False
            if obj_total == 0:
                return

        if len(json_response) > 0:
            for obj in json_response:
                obj_count += 1
                object_list.append(obj['name'].encode('utf-8'))
                last_object = object_list[-1]
                if obj_count == obj_total:
                    return object_list


#----------------------------------------------------------------------
def download_objects(path, dlCount):
    global auth_data
    global authority
    dl_dir = args.dir
    container = args.cont
    full_path = (dl_dir + '/' + container + '/' + path)
    dir_path = os.path.dirname(full_path)
    if args.quiet:
        pass
    elif args.verbose:
        print ("%s of %s: %s" % (dlCount, obj_total, full_path))
    else:
        print ("# D/L of object: %s of %s                        \r" % (dlCount, obj_total)),
    try:
        os.makedirs(dir_path)
    except OSError:
        pass

    path = path.decode('utf-8', 'replace')
    url = (authority + '/' + urllib2.quote(container) + '/' + path)
    headers = {'X-Auth-Token': auth_data['token'],  'Connection': 'Keep-Alive'}
    req_obj = Requests(container)
    req_obj.run_queries(url, headers, requests.get, streamVal='True', data=full_path)


#----------------------------------------------------------------------
def delete_objects(path, dlCount):
    global auth_data
    global authority
    container = args.cont
    path = path.decode('utf-8', 'replace')
    if args.quiet:
        pass
    elif args.verbose:
        print ("%s of %s: %s" % (dlCount, obj_total, path))
    else:
        print ("# Deleting: %s of %s                             \r" % (dlCount, obj_total)),

    url = (authority + '/' + urllib2.quote(container) + '/' + path)
    headers = {'X-Auth-Token': auth_data['token'],  'Connection': 'Keep-Alive'}
    req_obj = Requests(container)
    req_obj.run_queries(url, headers, requests.delete)


#----------------------------------------------------------------------
def delete_container():
    global auth_data
    global authority
    container = args.cont.decode('utf-8', 'replace')
    if args.quiet:
        pass
    elif args.verbose:
        print ("Deleting %s container" % (container))
    time.sleep(2)
    url = (authority + '/' + urllib2.quote(container))
    headers = {'X-Auth-Token': auth_data['token'],  'Connection': 'Keep-Alive'}
    req_obj = Requests(container)
    req_obj.run_queries(url, headers, requests.delete)


#----------------------------------------------------------------------
def upload_objects(path, progress_count):
    global auth_data
    global authority
    global obj_total
    container = args.cont
    if args.quiet:
        pass
    elif args.verbose:
        print ("%s of %s: %s" % (progress_count, obj_total, path))
    else:
        print ("# U/L of objects: %s of %s                       \r" % (progress_count, obj_total)),

    url_file = path
    if url_file.startswith('/'):
        url_file = url_file.strip('/')
    if url_file.startswith('./'):
        url_file = url_file.strip('./')
    if url_file.startswith('../'):
        url_file = url_file.strip('../')
    url = (authority + '/' + urllib2.quote(container) + '/' + url_file)
    headers = {'X-Auth-Token': auth_data['token'],  'Connection': 'Keep-Alive'}
    req_obj = Requests(container)
    resp = req_obj.run_queries(url, headers, requests.put, streamVal='True', data=path)
    return url, headers, resp


#----------------------------------------------------------------------
def create_container():
    global auth_data
    global authority
    container = args.cont
    if args.quiet:
        pass
    elif args.verbose:
        print "Creating %s container" % container

    url = (authority + '/' + urllib2.quote(container))
    headers = {'X-Auth-Token': auth_data['token'],  'Connection': 'Keep-Alive'}
    req_obj = Requests(container)
    req_obj.run_queries(url, headers, requests.put)
    time.sleep(2)


#----------------------------------------------------------------------
def collect_file_data(rootdir):
    global utf8log
    local_files = []
    for folder, subs, files in os.walk(rootdir):
        for filename in files:
            path = (os.path.join(folder, filename))
            try:
                local_files.append(path.encode('utf-8'))
            except:
                if args.logfile:
                    with open(args.logfile, "a") as log:
                        log.write(path)
                print "Non-UTF-8: %s" % path
                pass
    return local_files


#----------------------------------------------------------------------
def human_read_size(num):
    for x in ['B', 'KB', 'MB', 'GB']:
        if num < 1024.0:
            return "%3.1f%s" % (num, x)
        num /= 1024.0
    return "%3.1f%s" % (num, 'TB')


#----------------------------------------------------------------------
def collect_container_headers(container):
    global auth_data
    global authority
    url = (authority + '/' + container)
    headers = {'X-Auth-Token': auth_data['token'],  'Connection': 'Keep-Alive', 'Content-Type': 'application/json'}
    req_obj = Requests(container)
    resp = req_obj.run_queries(url, headers, requests.head)
    cont_header_dict = resp.headers
    cont_count = (int(cont_header_dict['x-container-object-count']))
    cont_size = (int(cont_header_dict['x-container-bytes-used']))
    human_size = human_read_size(cont_size)
    print_statement = ("Obj#: %-9s  Size: %9s  Name: %s" % (cont_count, human_size, container))
    return print_statement


#----------------------------------------------------------------------
def collect_object_headers(obj):
    global auth_data
    global authority
    container = args.cont
    obj = obj.decode('utf-8', 'replace')
    url = (authority + '/' + container + '/' + obj)
    headers = {'X-Auth-Token': auth_data['token'],  'Connection': 'Keep-Alive', 'Content-Type': 'application/json'}
    req_obj = Requests(container)
    resp = req_obj.run_queries(url, headers, requests.head)
    header_dict = resp.headers
    obj_date = (header_dict['last-modified'])
    obj_size = (int(header_dict['content-length']))
    human_size = human_read_size(obj_size)
    print_statement = ("%-29s  %9s  %s" % (obj_date, human_size, obj))
    return print_statement


#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#  
if __name__ == "__main__":
    #-- For testing ranges via argparse, choices floods the screen with numbers --
    class IntRange(object):
        def __init__(self, start, stop=None):
            if stop is None:
                start, stop = 0, start
            self.start, self.stop = start, stop

        def __call__(self, value):
            value = int(value)
            if value < self.start or value >= self.stop:
                raise argparse.ArgumentTypeError('value outside of range')
            return value

    #----------------------------------------------------------------------
    def init_sub_proc():
        signal.signal(signal.SIGINT, signal.SIG_IGN)

    #----------------------------------------------------------------------
    def job_spooler(target_def, obj_list):
        manager = multiprocessing.Manager()
        manager.Queue()
        tasks = manager.Queue()
        results = manager.Queue()

        consumers = [Consumer(tasks, results) for i in xrange(args.proc)]
        for w in consumers:
            w.start()

        num_jobs = len(obj_list)

        progress_count = 0
        for var in obj_list:
            progress_count += 1
            tasks.put(Task(var, progress_count, target_def))

        # Add a poison pill for each consumer
        for i in xrange(args.proc):
            tasks.put(None)

        while num_jobs:
            results.get()
            num_jobs -= 1

    #----------------------------------------------------------------------
    def map_print_spooler(target_object, obj_list):
        pool = multiprocessing.Pool(args.proc, init_sub_proc)
        sub_list = []
        for sub_item in obj_list:
            sub_list.append(sub_item)
            if len(sub_list) == args.proc or sub_item == obj_list[-1]:
                sub_detail_list = pool.map(target_object, sub_list)
                for sub_detail in sub_detail_list:
                    print sub_detail
                sub_list[:] = []
        pool.close()
        pool.join()

    #------ Calculate Cpu Info ----------------------------------------------
    cpu_num = int(multiprocessing.cpu_count())
    up_dn_def = (cpu_num * 3)
    ls_del_def = (cpu_num * 15)

    #------ Passed Arguments ------------------------------------------------
    parser = argparse.ArgumentParser(description='Command line options')
    parser.add_argument('-u', '--user', required=True, help='Account Username')
    parser.add_argument('-k', '--key', required=True, help='Account API-KEY')
    parser.add_argument('-r', '--reg', required=True, choices=['dfw', 'iad', 'lon', 'ord', 'syd', 'hkg'],
                        help='Region / DataCenter')
    parser.add_argument('-s', '--snet', action='store_true',
                        help='Use ServiceNet, aka 10.x.x.x addresses, instead of public')
    display_group = parser.add_mutually_exclusive_group()
    display_group.add_argument('-v', '--verbose', action='store_true', help='Show more data')
    display_group.add_argument('-q', '--quiet', action='store_true', help='Show only errors')
    subparsers = parser.add_subparsers(dest='subparser_name', help='Action sub-command choices')
    parser_dn = subparsers.add_parser('dn', help='Download from Cloud Files to this machine')
    parser_up = subparsers.add_parser('up', help='Upload from this machine to Cloud Files')
    parser_ls = subparsers.add_parser('ls', help='List objects in selected Cloud Files container')
    parser_del = subparsers.add_parser('del', help='Delete all objects and the container. Type carefully!')
    parser_del_from_file = subparsers.add_parser('del_from_file',
                                                 help='Delete all objects and the container. Type carefully!')
    parser_copy = subparsers.add_parser('copy', help='Copy all objects from one container to another container.')
    parser_dn.add_argument('-d', '--dir', required=True, help='The base directory that all files will be put into')
    parser_dn.add_argument('-c', '--cont', required=True, help='Container name to download files from')
    parser_dn.add_argument('-p', '--proc', type=IntRange(1, 201), default=up_dn_def,
                           help='1-200 concurrent procs, (default: %(default)s)')
    parser_up.add_argument('-d', '--dir', required=True,
                           help='The directory and everything recursively added to specified container')
    parser_up.add_argument('-c', '--cont', required=True, help='Container name to download files from.')
    parser_up.add_argument('-p', '--proc', type=IntRange(1, 101), default=up_dn_def,
                           help='1-100 concurrent procs, (default: %(default)s)')
    parser_up.add_argument('-l', '--logfile', help='/path/to/logfile.txt to append local utf-8 fail messages to')
    parser_ls.add_argument('-l', '--long', action='store_true',
                           help='Long or detailed info, like from the *nix commandline')
    parser_ls.add_argument('-g', '--grep', help='Filter results. Handy with the -l flag')
    parser_ls.add_argument('-c', '--cont', help='Container name to list objects from')
    parser_ls.add_argument('-p', '--proc', type=IntRange(1, 401), default=ls_del_def,
                           help='1-400 concurrent procs, (default: %(default)s)')
    parser_del.add_argument('-c', '--cont', required=True, help='Container name to delete.')
    parser_del.add_argument('-p', '--proc', type=IntRange(1, 401), default=ls_del_def,
                            help='1-400 concurrent procs, (default: %(default)s)')
    parser_del_from_file.add_argument('-c', '--cont', required=True,
                                      help='Container name containing objects to delete.')
    parser_del_from_file.add_argument('-f', '--file', required=True, help='objects in a file to pull in and delete')
    parser_del_from_file.add_argument('-p', '--proc', type=IntRange(1, 401), default=ls_del_def,
                                      help='1-400 concurrent procs, (default: %(default)s)')

    args = parser.parse_args()
    args.reg = args.reg.upper()
    dict_args = vars(args)

    # ___ Authenticate & build Authority portion of URL ___
    auth_data = Authentication().cloud_files_service(dict_args)
    region = auth_data['region'].split('/')[2]
    authority = ('https://' + region + '/v1/' + auth_data['tenantid'])

    # ___ Download from container to localhost ___
    if args.subparser_name == 'dn':
        if not os.path.isdir(args.dir):
            print ("\nBase directory non-existent or not writable: " + args.dir)
            exit()
        object_list = collect_container_objects()
        if object_list:
            job_spooler(download_objects, object_list)

    # ___ Container or Object lists ___
    if args.subparser_name == 'ls':
        if args.cont:
            object_list = collect_container_objects()
            if object_list:
                if args.long:
                    sublist = []
                    for subitem in object_list:
                        if args.grep and not args.grep in subitem:
                            continue
                        sublist.append(subitem)
                    map_print_spooler(collect_object_headers, sublist)
                else:
                    for obj in object_list:
                        if args.grep and not args.grep in obj:
                            continue
                        print obj
        else:
            container_list = collect_containers()
            if container_list:
                if args.long:
                    sublist = []
                    for subitem in container_list:
                        if args.grep and not args.grep in subitem:
                            continue
                        sublist.append(subitem)
                    map_print_spooler(collect_container_headers, sublist)
                else:
                    for cont in container_list:
                        if args.grep and not args.grep in cont:
                            continue
                        print cont

    # ___ Upload from localhost to Container ___
    if args.subparser_name == 'up':
        utf8log = args.logfile
        local_files = collect_file_data(args.dir)
        obj_total = len(local_files)
        create_container()
        job_spooler(upload_objects, local_files)

    # ___ Delete container and objects ___
    if args.subparser_name == 'del':
        object_list = collect_container_objects()
        if object_list:
            job_spooler(delete_objects, object_list)
        delete_container()

    # ___ Delete container and objects ___
    object_list = []
    if args.subparser_name == 'batchdel':
        with open(args.file, 'r') as objline:
            for line in objline:
                object_list.append(line.rstrip('\n'))
        obj_total = len(object_list)
        if object_list:
            job_spooler(delete_objects, object_list)
